{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fad00390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer 논문은 주로 다음과 같은 흐름으로 구성되어 있습니다:\n",
      "\n",
      "1. **서론 (Introduction)**:\n",
      "   - 기존의 순환 신경망(RNN) 및 합성곱 신경망(CNN) 기반의 시퀀스 변환 모델의 한계를 언급하며, 주의 메커니즘에 기반한 새로운 아키텍처인 Transformer를 제안합니다. 이 모델은 RNN이나 CNN을 사용하지 않고도 뛰어난 성능을 발휘할 수 있음을 강조합니다 [doc3].\n",
      "\n",
      "2. **배경 (Background)**:\n",
      "   - 주의 메커니즘의 필요성과 효과를 설명합니다. 특히, 긴 거리의 의존성을 모델링하는 데 있어 주의 메커니즘이 어떻게 기여하는지를 논의합니다 [doc1][doc2].\n",
      "\n",
      "3. **모델 아키텍처 (Model Architecture)**:\n",
      "   - Transformer의 구조를 상세히 설명합니다. 이 모델은 인코더와 디코더 스택으로 구성되어 있으며, 각 스택은 다중 머리 자기 주의(multi-head self-attention)와 완전 연결 피드포워드 네트워크로 이루어져 있습니다. 또한 잔차 연결(residual connection)과 층 정규화(layer normalization)도 포함되어 있습니다 [doc1].\n",
      "\n",
      "4. **학습 및 최적화 (Training and Optimization)**:\n",
      "   - 모델을 학습시키기 위한 하이퍼파라미터, 최적화 기법(Adam optimizer 사용) 및 정규화 방법에 대해 설명합니다. 특히, 레이블 스무딩(label smoothing)과 드롭아웃(dropout) 기법을 적용하여 모델의 일반화 성능을 향상시키는 방법을 논의합니다 [doc4][doc5].\n",
      "\n",
      "5. **실험 및 결과 (Experiments and Results)**:\n",
      "   - WMT 2014 영어-독일어 및 영어-프랑스어 번역 과제에서의 실험 결과를 제시하며, Transformer가 다른 모델들보다 우수한 성능을 발휘했음을 보여줍니다. BLEU 점수와 학습 비용을 비교하여 Transformer의 효율성을 강조합니다 [doc4][doc5].\n",
      "\n",
      "6. **결론 (Conclusion)**:\n",
      "   - Transformer의 성능과 향후 연구 방향에 대한 논의를 포함합니다. 다른 입력 및 출력 모달리티에 대한 적용 가능성, 그리고 더 복잡한 문제를 해결하기 위한 연구 계획을 제안합니다 [doc2][doc3].\n",
      "\n",
      "이 논문은 전체적으로 주의 메커니즘을 기반으로 한 혁신적인 모델 아키텍처를 제안하고, 그 성능을 다양한 실험을 통해 입증하는 흐름으로 구성되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "deployment = \"gpt-4o-mini\"\n",
    "\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "search_index = \"transformer-paper-index\"\n",
    "\n",
    "show_citations = True\n",
    "\n",
    "# Initialize Azure OpenAI client with key-based authentication\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=\"2025-01-01-preview\",\n",
    ")\n",
    "\n",
    "# Prepare the chat prompt\n",
    "chat_prompt = [\n",
    "    {\n",
    "        \"role\" : \"system\",\n",
    "        \"content\" : \"당신은 transformer 논문에 대해 잘 알고있는 교육자입니다.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : \"transformer 논문의 전반적인 흐름을 알려줄래?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Include speech result if speech is enabled\n",
    "messages = chat_prompt\n",
    "\n",
    "# Generate the completion\n",
    "completion = client.chat.completions.create(\n",
    "    model=deployment,\n",
    "    messages=messages,\n",
    "    max_tokens=6553,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    "    extra_body={\n",
    "      \"data_sources\": [{\n",
    "          \"type\": \"azure_search\",\n",
    "          \"parameters\": {\n",
    "            \"endpoint\": f\"{search_endpoint}\",\n",
    "            \"index_name\": \"transformer-paper-index\",\n",
    "            \"semantic_configuration\": \"default\",\n",
    "            \"query_type\": \"semantic\",\n",
    "            \"fields_mapping\": {},\n",
    "            \"in_scope\": True,\n",
    "            # \"role_information\": \"사용자가 정보를 찾는 데 도움이 되는 AI 도우미입니다.\",\n",
    "            \"filter\": None,\n",
    "            \"strictness\": 3,\n",
    "            \"top_n_documents\": 5,\n",
    "            \"authentication\": {\n",
    "              \"type\": \"api_key\",\n",
    "              \"key\": f\"{search_key}\"\n",
    "            }\n",
    "          }\n",
    "        }]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2cd5213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer 논문은 주로 다음과 같은 흐름으로 구성되어 있습니다:\n",
      "\n",
      "1. **서론 (Introduction)**:\n",
      "   - 기존의 순환 신경망(RNN) 및 합성곱 신경망(CNN) 기반의 시퀀스 변환 모델의 한계를 언급하며, 주의 메커니즘에 기반한 새로운 아키텍처인 Transformer를 제안합니다. 이 모델은 RNN이나 CNN을 사용하지 않고도 뛰어난 성능을 발휘할 수 있음을 강조합니다 [doc3].\n",
      "\n",
      "2. **배경 (Background)**:\n",
      "   - 주의 메커니즘의 필요성과 효과를 설명합니다. 특히, 긴 거리의 의존성을 모델링하는 데 있어 주의 메커니즘이 어떻게 기여하는지를 논의합니다 [doc1][doc2].\n",
      "\n",
      "3. **모델 아키텍처 (Model Architecture)**:\n",
      "   - Transformer의 구조를 상세히 설명합니다. 이 모델은 인코더와 디코더 스택으로 구성되어 있으며, 각 스택은 다중 머리 자기 주의(multi-head self-attention)와 완전 연결 피드포워드 네트워크로 이루어져 있습니다. 또한 잔차 연결(residual connection)과 층 정규화(layer normalization)도 포함되어 있습니다 [doc1].\n",
      "\n",
      "4. **학습 및 최적화 (Training and Optimization)**:\n",
      "   - 모델을 학습시키기 위한 하이퍼파라미터, 최적화 기법(Adam optimizer 사용) 및 정규화 방법에 대해 설명합니다. 특히, 레이블 스무딩(label smoothing)과 드롭아웃(dropout) 기법을 적용하여 모델의 일반화 성능을 향상시키는 방법을 논의합니다 [doc4][doc5].\n",
      "\n",
      "5. **실험 및 결과 (Experiments and Results)**:\n",
      "   - WMT 2014 영어-독일어 및 영어-프랑스어 번역 과제에서의 실험 결과를 제시하며, Transformer가 다른 모델들보다 우수한 성능을 발휘했음을 보여줍니다. BLEU 점수와 학습 비용을 비교하여 Transformer의 효율성을 강조합니다 [doc4][doc5].\n",
      "\n",
      "6. **결론 (Conclusion)**:\n",
      "   - Transformer의 성능과 향후 연구 방향에 대한 논의를 포함합니다. 다른 입력 및 출력 모달리티에 대한 적용 가능성, 그리고 더 복잡한 문제를 해결하기 위한 연구 계획을 제안합니다 [doc2][doc3].\n",
      "\n",
      "이 논문은 전체적으로 주의 메커니즘을 기반으로 한 혁신적인 모델 아키텍처를 제안하고, 그 성능을 다양한 실험을 통해 입증하는 흐름으로 구성되어 있습니다.\n",
      "\n",
      "Citations : \n",
      "Doc no.1\n",
      " Title: Vaswani, A. et al. (2017)). Attention is all you need.pdf\n",
      " URL: https://8ai028storage.blob.core.windows.net/fileupload-transformer-paper-index/Vaswani,%20A.%20et%20al.%20(2017)).%20Attention%20is%20all%20you%20need.pdf\n",
      "\n",
      "Doc no.2\n",
      " Title: Vaswani, A. et al. (2017)). Attention is all you need.pdf\n",
      " URL: https://8ai028storage.blob.core.windows.net/fileupload-transformer-paper-index/Vaswani,%20A.%20et%20al.%20(2017)).%20Attention%20is%20all%20you%20need.pdf\n",
      "\n",
      "Doc no.3\n",
      " Title: Vaswani, A. et al. (2017)). Attention is all you need.pdf\n",
      " URL: https://8ai028storage.blob.core.windows.net/fileupload-transformer-paper-index/Vaswani,%20A.%20et%20al.%20(2017)).%20Attention%20is%20all%20you%20need.pdf\n",
      "\n",
      "Doc no.4\n",
      " Title: Vaswani, A. et al. (2017)). Attention is all you need.pdf\n",
      " URL: https://8ai028storage.blob.core.windows.net/fileupload-transformer-paper-index/Vaswani,%20A.%20et%20al.%20(2017)).%20Attention%20is%20all%20you%20need.pdf\n",
      "\n",
      "Doc no.5\n",
      " Title: Vaswani, A. et al. (2017)). Attention is all you need.pdf\n",
      " URL: https://8ai028storage.blob.core.windows.net/fileupload-transformer-paper-index/Vaswani,%20A.%20et%20al.%20(2017)).%20Attention%20is%20all%20you%20need.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)\n",
    "\n",
    "if show_citations :\n",
    "    print(\"\\nCitations : \")\n",
    "\n",
    "    citation_title = completion.choices[0].message.context[\"citations\"][0][\"title\"]\n",
    "    citation_url = completion.choices[0].message.context[\"citations\"][0][\"url\"]\n",
    "\n",
    "    for c in range(len(completion.choices[0].message.context[\"citations\"])) :\n",
    "        print(f\"Doc no.{c+1}\")\n",
    "        print(\" Title: \" + citation_title + \"\\n URL: \" + citation_url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af332584",
   "metadata": {},
   "outputs": [],
   "source": [
    "Citations : \n",
    "ChatCompletion(id='c5776472-9aa4-49e1-88b9-c122e5b465a8', \n",
    "               choices=[Choice(finish_reason='stop', \n",
    "                               index=0, logprobs=None, \n",
    "                               message=ChatCompletionMessage(content='Transformer 논문은 다음과 같은 주요 흐름으로 구성되어 있습니다:\\n\\n1. **서론**: 논문은 기존의 복잡한 순환 신경망(RNN) 및 컨볼루션 신경망 기반의 시퀀스 변환 모델에 대한 배경을 설명하고, 이를 대체할 새로운 아키텍처인 Transformer를 제안합니다. Transformer는 전통적인 RNN이나 컨볼루션을 사용하지 않고, 오로지 주의(attention) 메커니즘에 기반하여 작동합니다 [doc2].\\n\\n2. **모델 아키텍처**: Transformer는 인코더-디코더 구조를 따릅니다. 인코더는 입력 시퀀스를 연속 표현으로 변환하고, 디코더는 이 표현을 기반으로 출력 시퀀스를 생성합니다. 각 인코더와 디코더는 여러 개의 층으로 구성되어 있으며, 각 층은 다중 헤드 주의 메커니즘과 완전 연결(feed-forward) 네트워크를 포함합니다 [doc1][doc2].\\n\\n3. **자기 주의 메커니즘**: Transformer의 핵심은 자기 주의(self-attention) 메커니즘으로, 이는 입력 시퀀스 내의 서로 다른 위치 간의 관계를 모델링하는 데 사용됩니다. 이를 통해 모델은 입력의 모든 위치를 고려하여 표현을 계산할 수 있습니다 [doc1][doc5].\\n\\n4. **위치 인코딩**: 모델 내에 순환이나 컨볼루션이 없기 때문에, 입력 토큰의 순서 정보를 인코딩하기 위해 위치 인코딩(positional encoding)을 추가합니다. 이 인코딩은 사인 및 코사인 함수의 조합으로 생성됩니다 [doc5].\\n\\n5. **훈련 및 결과**: 논문에서는 Transformer 모델을 WMT 2014 영어-독일어 및 영어-프랑스어 번역 작업에 적용한 결과를 제시합니다. Transformer는 기존 모델들보다 우수한 BLEU 점수를 기록하며, 훈련 시간 또한 획기적으로 단축되었습니다 [doc3][doc4].\\n\\n6. **결론**: 마지막으로, Transformer 모델의 성공적인 결과를 요약하고, 향후 연구 방향으로는 다른 입력 및 출력 모달리티에의 적용, 대규모 입력 및 출력 처리, 생성 과정의 비순차적 처리 등을 제안합니다 [doc4].\\n\\n이러한 흐름을 통해 Transformer는 기존의 시퀀스 모델링 접근법을 혁신적으로 개선하였음을 보여줍니다.',\n",
    "                                                             refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, end_turn=True,\n",
    "                                                             context={'citations': [{'content': 'constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n\\n3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n\\n2\\n\\n\\n\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\n\\n\\naHR0cHM6Ly84YWkwMjhzdG9yYWdlLmJsb2IuY29yZS53aW5kb3dzLm5ldC9maWxldXBsb2FkLXRyYW5zZm9ybWVyLXBhcGVyLWluZGV4L1Zhc3dhbmksJTIwQS4lMjBldCUyMGFsLiUyMCgyMDE3KSkuJTIwQXR0ZW50aW9uJTIwaXMlMjBhbGwlMjB5b3UlMjBuZWVkLnBkZg2',\n",
    "                                                                                     'title': 'Vaswani, A. et al. (2017)). Attention is all you need.pdf',\n",
    "                                                                                     'url': 'https://8ai028storage.blob.core.windows.net/fileupload-transformer-paper-index/Vaswani,%20A.%20et%20al.%20(2017)).%20Attention%20is%20all%20you%20need.pdf',\n",
    "                                                                                     'filepath': 'Vaswani, A. et al. (2017)). Attention is all you need.pdf', 'chunk_id': '0'},\n",
    "                                                                                     {'content': '\\nProvided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\n\\nscholarly works.\\n\\nAttention Is All You Need\\n\\nAshish Vaswani∗\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
